{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model\n",
    "from statistics import mean, stdev\n",
    "from sklearn.preprocessing import scale \n",
    "from keras.utils import to_categorical\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOXL = pd.read_csv('soxl_new.csv') #ETF growth cycle\n",
    "TQQQ = pd.read_csv('tqqq_new.csv') #3X Index\n",
    "MU = pd.read_csv('mu_new.csv') #high Beta\n",
    "AMD = pd.read_csv('amd_new.csv') # high beta\n",
    "NFLX = pd.read_csv('nflx_new.csv') #High growth\n",
    "AMZN = pd.read_csv('amzn_new.csv') #High growth\n",
    "V = pd.read_csv('visa_new.csv') #low volalitity\n",
    "NVDA = pd.read_csv('nvda_new.csv') #high growth\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['abv_1.07', 'abv_1.04', 'abv_1.02', 'bel_1.02'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NFLX['tar_3best_class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.017512  ],\n",
       "        [ 0.94859576],\n",
       "        [68.282555  ],\n",
       "        ...,\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.9890316 ],\n",
       "        [ 0.9737122 ],\n",
       "        [64.542854  ],\n",
       "        ...,\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 1.0057704 ],\n",
       "        [ 0.9915572 ],\n",
       "        [62.478878  ],\n",
       "        ...,\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.96699005],\n",
       "        [ 1.0701464 ],\n",
       "        [21.292788  ],\n",
       "        ...,\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.98630136],\n",
       "        [ 1.0273151 ],\n",
       "        [35.765232  ],\n",
       "        ...,\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 1.0519959 ],\n",
       "        [ 0.9919561 ],\n",
       "        [46.111443  ],\n",
       "        ...,\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['Day_previous_roi','ma10','rsi10','ma20','rsi20','ma_chg20',\n",
    "            'ma60','rsi60','ma200','rsi200','obv','macd_diff','ma_chg10',\n",
    "            'macd_diff_hist','aroon_diff','slope60','r_sqr_60','ma_chg60',\n",
    "            'slope10','r_sqr_10','slope5','r_sqr_5','stDev20','ma_chg200',\n",
    "            'rsi_chg10','rsi_chg20','rsi_chg60','rsi_chg200',\n",
    "            'percent_down','sine','leadsine','tsf10','tsf20','tsf60','tsf200',\n",
    "            'up_dwn_prev','shawman','hammer','semi_pk_pr']\n",
    "top_feats = ['ma200',               \n",
    "             'macd_diff_hist',      \n",
    "             'tsf200',              \n",
    "             'r_sqr_60',            \n",
    "              'slope60']#,             \n",
    "#              'macd_diff',           \n",
    "#              'tsf60',               \n",
    "#              'slope10',             \n",
    "#              'r_sqr_10',            \n",
    "#              'percent_down',        \n",
    "#              'rsi60',               \n",
    "#              'obv']\n",
    "#Set stock or dataframe\n",
    "df_cln = NFLX\n",
    "target_name = 'tar_3best_class'\n",
    "#.75 make a 25/75 split\n",
    "stop = round(.80*len(df_cln))\n",
    "\n",
    "#set features\n",
    "\n",
    "features = df_cln[features].values\n",
    "top_features = df_cln[top_feats].values\n",
    "targets = df_cln[target_name].values\n",
    "arr = []\n",
    "end = targets.shape[0]\n",
    "for i in range(end):\n",
    "    #print(targets[i])\n",
    "    if targets[i] == 'bel_1.02':\n",
    "        arr.append([1,0,0,0])\n",
    "    elif targets[i] == 'abv_1.02':\n",
    "        arr.append([0,1,0,0])\n",
    "    elif targets[i] == 'abv_1.04':\n",
    "        arr.append([0,0,1,0])\n",
    "    elif targets[i] == 'abv_1.07':\n",
    "        arr.append([0,0,0,1])\n",
    "target_int = arr\n",
    "\n",
    "feature_train = features[:stop]\n",
    "feature_test = features[stop:]\n",
    "\n",
    "top_feat_train  = top_features[:stop]\n",
    "top_feat_test = top_features[stop:]\n",
    "\n",
    "target_test_int = target_int[stop:]\n",
    "target_train_int = target_int[:stop]\n",
    "\n",
    "\n",
    "#set my targets\n",
    "\n",
    "\n",
    "# feature_train = np.array(feature_train)\n",
    "# target_train = np.array(target_train)\n",
    "# feature_test = np.array(feature_test)\n",
    "# target_test = np.array(target_test)\n",
    "\n",
    "\n",
    "feature_train.reshape(feature_train.shape[0], \n",
    "                      feature_train.shape[1],\n",
    "                      1).astype( 'float32' )\n",
    "\n",
    "feature_test.reshape(feature_test.shape[0], \n",
    "                      feature_test.shape[1],\n",
    "                      1).astype( 'float32' )\n",
    "#print(feature_train.shape,target_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# categorical_labels = to_categorical(target_int, num_classes=4)\n",
    "# categorical_labels\n",
    "#target_train_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Standardize the train and test features\n",
    "# scaled_train_features = scale(feature_train)\n",
    "# scaled_test_features = scale(feature_test)\n",
    "# # Create the model\n",
    "# def baseline_model():\n",
    "#     model_1 = Sequential()\n",
    "#     model_1.add(Dense(200, input_dim=scaled_train_features.shape[1], activation='relu'))\n",
    "#     #model_1.add(Dropout(0.25))\n",
    "#     model_1.add(Dense(200, activation='relu'))\n",
    "#     #model_1.add(Dropout(0.25))\n",
    "#     model_1.add(Dense(100, activation='relu'))\n",
    "#     model_1.add(Dense(10, activation='relu'))\n",
    "#     model_1.add(Dense(4, activation='softmax'))\n",
    "    \n",
    "#     model_1.compile(loss= 'categorical_crossentropy' ,\n",
    "#                 optimizer= 'adam' ,\n",
    "#                 metrics=[ 'accuracy' ])\n",
    "#     return model_1\n",
    "\n",
    "# # Fit the model\n",
    "\n",
    "# history = KerasClassifier(build_fn=baseline_model, epochs=40, batch_size=200, verbose=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "# results = cross_val_score(history,scaled_train_features , target_train, cv=kfold)\n",
    "# print(results.mean())\n",
    "\n",
    "# %matplotlib inline\n",
    "# plt.plot(history.history['acc'],'b') \n",
    "# plt.plot(history.history['val_acc'],'r')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = baseline_model()\n",
    "# model.fit(feature_train,target_train, epochs=40, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_test.unique()\n",
    "# Standardize the train and test features\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train = min_max_scaler.fit_transform(feature_train)\n",
    "X_test = min_max_scaler.fit_transform(feature_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "top_X_train = min_max_scaler.fit_transform(top_feat_train)\n",
    "top_X_test = min_max_scaler.fit_transform(top_feat_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from keras.models import Model, load_model\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 1,796\n",
      "Trainable params: 1,540\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Standardize the train and test features\n",
    "K.clear_session()\n",
    "target_test = np.array(target_test_int)\n",
    "target_train = np.array(target_train_int)\n",
    "# scaled_train_features = scale(feature_train)\n",
    "# scaled_test_features = scale(feature_test)\n",
    "# Create the model\n",
    "K.clear_session()\n",
    "\n",
    "inputs = Input(shape=(top_X_train.shape[1], ))\n",
    "x1 = Dense(128)(inputs)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Activation('relu')(x1)\n",
    "x1 = Dropout(0.5)(x1)\n",
    "\n",
    "# x2 = Dense(16, activation='relu')(x1)\n",
    "# x2 = BatchNormalization()(x2)\n",
    "# x2 = Activation('relu')(x2)\n",
    "\n",
    "# x3 = Dense(16, activation='relu')(x2)\n",
    "# x3 = BatchNormalization()(x3)\n",
    "# x3 = Activation('relu')(x3)\n",
    "\n",
    "# x4 = Dense(8, activation='relu')(x3)\n",
    "# x4 = BatchNormalization()(x4)\n",
    "# x4 = Activation('relu')(x4)\n",
    "\n",
    "# x5 = Dense(16, activation='relu')(x4)\n",
    "# x5 = BatchNormalization()(x5)\n",
    "# x5 = Activation('relu')(x5)\n",
    "\n",
    "\n",
    "# x6 = Dense(32, activation='relu')(x5)\n",
    "# x6 = BatchNormalization()(x6)\n",
    "# x6 = Activation('relu')(x6)\n",
    "\n",
    "\n",
    "# x7 = Dense(64, activation='relu')(x6)\n",
    "# x7 = BatchNormalization()(x7)\n",
    "# x7 = Activation('relu')(x7)\n",
    "\n",
    "x = Dense(4, activation='softmax')(x1)\n",
    "\n",
    "checkpoint = ModelCheckpoint('3_layer_dense.h5',\n",
    "                             monitor='val_loss',\n",
    "                             save_best_only=True)\n",
    "cb = [checkpoint]\n",
    "\n",
    "# this compiles our model so it is ready to fit\n",
    "model = Model(inputs, x)\n",
    "model.compile(loss= 'categorical_crossentropy' ,\n",
    "                optimizer= 'adam' ,\n",
    "                metrics=[ 'accuracy' ])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2645 samples, validate on 467 samples\n",
      "Epoch 1/100\n",
      "2645/2645 [==============================] - 1s 494us/step - loss: 1.6835 - acc: 0.2469 - val_loss: 1.4954 - val_acc: 0.1349\n",
      "Epoch 2/100\n",
      "2645/2645 [==============================] - 0s 50us/step - loss: 1.5413 - acc: 0.2563 - val_loss: 1.4504 - val_acc: 0.2441\n",
      "Epoch 3/100\n",
      "2645/2645 [==============================] - 0s 30us/step - loss: 1.5222 - acc: 0.2699 - val_loss: 1.4023 - val_acc: 0.2719\n",
      "Epoch 4/100\n",
      "2645/2645 [==============================] - 0s 51us/step - loss: 1.5103 - acc: 0.2665 - val_loss: 1.4080 - val_acc: 0.2463\n",
      "Epoch 5/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.5254 - acc: 0.2480 - val_loss: 1.4041 - val_acc: 0.2355\n",
      "Epoch 6/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.4828 - acc: 0.2677 - val_loss: 1.3979 - val_acc: 0.2334\n",
      "Epoch 7/100\n",
      "2645/2645 [==============================] - 0s 53us/step - loss: 1.4722 - acc: 0.2817 - val_loss: 1.3723 - val_acc: 0.2655\n",
      "Epoch 8/100\n",
      "2645/2645 [==============================] - 0s 40us/step - loss: 1.4706 - acc: 0.2771 - val_loss: 1.3670 - val_acc: 0.2784\n",
      "Epoch 9/100\n",
      "2645/2645 [==============================] - 0s 46us/step - loss: 1.4615 - acc: 0.2703 - val_loss: 1.3669 - val_acc: 0.2677\n",
      "Epoch 10/100\n",
      "2645/2645 [==============================] - 0s 45us/step - loss: 1.4448 - acc: 0.2745 - val_loss: 1.3642 - val_acc: 0.3019\n",
      "Epoch 11/100\n",
      "2645/2645 [==============================] - 0s 53us/step - loss: 1.4509 - acc: 0.2669 - val_loss: 1.3574 - val_acc: 0.3062\n",
      "Epoch 12/100\n",
      "2645/2645 [==============================] - 0s 74us/step - loss: 1.4367 - acc: 0.2783 - val_loss: 1.3524 - val_acc: 0.2869\n",
      "Epoch 13/100\n",
      "2645/2645 [==============================] - 0s 40us/step - loss: 1.4330 - acc: 0.2745 - val_loss: 1.3518 - val_acc: 0.2677\n",
      "Epoch 14/100\n",
      "2645/2645 [==============================] - 0s 56us/step - loss: 1.4352 - acc: 0.2767 - val_loss: 1.3501 - val_acc: 0.2570\n",
      "Epoch 15/100\n",
      "2645/2645 [==============================] - 0s 31us/step - loss: 1.4193 - acc: 0.2802 - val_loss: 1.3691 - val_acc: 0.2719\n",
      "Epoch 16/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.4117 - acc: 0.2870 - val_loss: 1.3651 - val_acc: 0.2634\n",
      "Epoch 17/100\n",
      "2645/2645 [==============================] - 0s 106us/step - loss: 1.4149 - acc: 0.2824 - val_loss: 1.3527 - val_acc: 0.2634\n",
      "Epoch 18/100\n",
      "2645/2645 [==============================] - 0s 80us/step - loss: 1.4044 - acc: 0.2949 - val_loss: 1.3646 - val_acc: 0.2591\n",
      "Epoch 19/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.4064 - acc: 0.2900 - val_loss: 1.3666 - val_acc: 0.2698\n",
      "Epoch 20/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3931 - acc: 0.2919 - val_loss: 1.3587 - val_acc: 0.2655\n",
      "Epoch 21/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3942 - acc: 0.2873 - val_loss: 1.3469 - val_acc: 0.2655\n",
      "Epoch 22/100\n",
      "2645/2645 [==============================] - 0s 54us/step - loss: 1.4031 - acc: 0.2888 - val_loss: 1.3503 - val_acc: 0.2655\n",
      "Epoch 23/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3988 - acc: 0.2896 - val_loss: 1.3567 - val_acc: 0.2805\n",
      "Epoch 24/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3852 - acc: 0.2820 - val_loss: 1.3583 - val_acc: 0.2891\n",
      "Epoch 25/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3887 - acc: 0.2979 - val_loss: 1.3440 - val_acc: 0.2719\n",
      "Epoch 26/100\n",
      "2645/2645 [==============================] - 0s 53us/step - loss: 1.3911 - acc: 0.2945 - val_loss: 1.3411 - val_acc: 0.2891\n",
      "Epoch 27/100\n",
      "2645/2645 [==============================] - 0s 40us/step - loss: 1.3874 - acc: 0.2900 - val_loss: 1.3481 - val_acc: 0.2784\n",
      "Epoch 28/100\n",
      "2645/2645 [==============================] - 0s 106us/step - loss: 1.3890 - acc: 0.2998 - val_loss: 1.3498 - val_acc: 0.2827\n",
      "Epoch 29/100\n",
      "2645/2645 [==============================] - 0s 45us/step - loss: 1.3847 - acc: 0.2896 - val_loss: 1.3657 - val_acc: 0.2655\n",
      "Epoch 30/100\n",
      "2645/2645 [==============================] - 0s 74us/step - loss: 1.3818 - acc: 0.3006 - val_loss: 1.3496 - val_acc: 0.3062\n",
      "Epoch 31/100\n",
      "2645/2645 [==============================] - 0s 45us/step - loss: 1.3740 - acc: 0.3025 - val_loss: 1.3520 - val_acc: 0.3084\n",
      "Epoch 32/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3768 - acc: 0.3081 - val_loss: 1.3584 - val_acc: 0.2719\n",
      "Epoch 33/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3814 - acc: 0.3081 - val_loss: 1.3660 - val_acc: 0.2741\n",
      "Epoch 34/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3820 - acc: 0.2953 - val_loss: 1.3594 - val_acc: 0.2848\n",
      "Epoch 35/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3799 - acc: 0.2957 - val_loss: 1.3643 - val_acc: 0.2805\n",
      "Epoch 36/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3805 - acc: 0.3002 - val_loss: 1.3672 - val_acc: 0.2891\n",
      "Epoch 37/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3782 - acc: 0.3051 - val_loss: 1.3572 - val_acc: 0.3126\n",
      "Epoch 38/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3708 - acc: 0.3138 - val_loss: 1.3597 - val_acc: 0.2998\n",
      "Epoch 39/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3659 - acc: 0.3214 - val_loss: 1.3638 - val_acc: 0.2998\n",
      "Epoch 40/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3717 - acc: 0.3051 - val_loss: 1.3559 - val_acc: 0.3169\n",
      "Epoch 41/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3702 - acc: 0.3168 - val_loss: 1.3562 - val_acc: 0.3084\n",
      "Epoch 42/100\n",
      "2645/2645 [==============================] - 0s 79us/step - loss: 1.3699 - acc: 0.3093 - val_loss: 1.3560 - val_acc: 0.3084\n",
      "Epoch 43/100\n",
      "2645/2645 [==============================] - 0s 72us/step - loss: 1.3730 - acc: 0.3100 - val_loss: 1.3594 - val_acc: 0.3169\n",
      "Epoch 44/100\n",
      "2645/2645 [==============================] - 0s 106us/step - loss: 1.3696 - acc: 0.3217 - val_loss: 1.3547 - val_acc: 0.2955\n",
      "Epoch 45/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3736 - acc: 0.3066 - val_loss: 1.3629 - val_acc: 0.2805\n",
      "Epoch 46/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3730 - acc: 0.3161 - val_loss: 1.3682 - val_acc: 0.2976\n",
      "Epoch 47/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3687 - acc: 0.3130 - val_loss: 1.3562 - val_acc: 0.2976\n",
      "Epoch 48/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3758 - acc: 0.3100 - val_loss: 1.3520 - val_acc: 0.2762\n",
      "Epoch 49/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3683 - acc: 0.3142 - val_loss: 1.3517 - val_acc: 0.3062\n",
      "Epoch 50/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3696 - acc: 0.3104 - val_loss: 1.3508 - val_acc: 0.3148\n",
      "Epoch 51/100\n",
      "2645/2645 [==============================] - 0s 106us/step - loss: 1.3743 - acc: 0.3108 - val_loss: 1.3470 - val_acc: 0.2848\n",
      "Epoch 52/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3757 - acc: 0.2922 - val_loss: 1.3470 - val_acc: 0.3062\n",
      "Epoch 53/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3739 - acc: 0.3108 - val_loss: 1.3370 - val_acc: 0.3148\n",
      "Epoch 54/100\n",
      "2645/2645 [==============================] - 0s 43us/step - loss: 1.3647 - acc: 0.3180 - val_loss: 1.3422 - val_acc: 0.3041\n",
      "Epoch 55/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3686 - acc: 0.3161 - val_loss: 1.3468 - val_acc: 0.3084\n",
      "Epoch 56/100\n",
      "2645/2645 [==============================] - 0s 106us/step - loss: 1.3666 - acc: 0.3248 - val_loss: 1.3533 - val_acc: 0.3084\n",
      "Epoch 57/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3661 - acc: 0.3157 - val_loss: 1.3548 - val_acc: 0.3084\n",
      "Epoch 58/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3646 - acc: 0.3217 - val_loss: 1.3543 - val_acc: 0.3105\n",
      "Epoch 59/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3679 - acc: 0.3164 - val_loss: 1.3624 - val_acc: 0.3105\n",
      "Epoch 60/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3655 - acc: 0.3274 - val_loss: 1.3542 - val_acc: 0.3062\n",
      "Epoch 61/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3648 - acc: 0.3308 - val_loss: 1.3562 - val_acc: 0.2848\n",
      "Epoch 62/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3644 - acc: 0.3206 - val_loss: 1.3676 - val_acc: 0.2677\n",
      "Epoch 63/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3682 - acc: 0.3028 - val_loss: 1.3732 - val_acc: 0.2805\n",
      "Epoch 64/100\n",
      "2645/2645 [==============================] - 0s 45us/step - loss: 1.3620 - acc: 0.3217 - val_loss: 1.3688 - val_acc: 0.2934\n",
      "Epoch 65/100\n",
      "2645/2645 [==============================] - 0s 106us/step - loss: 1.3632 - acc: 0.3229 - val_loss: 1.3682 - val_acc: 0.2955\n",
      "Epoch 66/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3653 - acc: 0.3323 - val_loss: 1.3671 - val_acc: 0.3019\n",
      "Epoch 67/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3638 - acc: 0.3233 - val_loss: 1.3698 - val_acc: 0.3019\n",
      "Epoch 68/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3661 - acc: 0.3217 - val_loss: 1.3665 - val_acc: 0.3169\n",
      "Epoch 69/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3640 - acc: 0.3108 - val_loss: 1.3611 - val_acc: 0.3105\n",
      "Epoch 70/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3650 - acc: 0.3198 - val_loss: 1.3608 - val_acc: 0.3105\n",
      "Epoch 71/100\n",
      "2645/2645 [==============================] - 0s 106us/step - loss: 1.3660 - acc: 0.3293 - val_loss: 1.3651 - val_acc: 0.3105\n",
      "Epoch 72/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3684 - acc: 0.3130 - val_loss: 1.3626 - val_acc: 0.3126\n",
      "Epoch 73/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3630 - acc: 0.3251 - val_loss: 1.3540 - val_acc: 0.3019\n",
      "Epoch 74/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3656 - acc: 0.3229 - val_loss: 1.3585 - val_acc: 0.3148\n",
      "Epoch 75/100\n",
      "2645/2645 [==============================] - 0s 78us/step - loss: 1.3645 - acc: 0.3282 - val_loss: 1.3647 - val_acc: 0.3105\n",
      "Epoch 76/100\n",
      "2645/2645 [==============================] - 0s 107us/step - loss: 1.3635 - acc: 0.3172 - val_loss: 1.3613 - val_acc: 0.3019\n",
      "Epoch 77/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3633 - acc: 0.3236 - val_loss: 1.3549 - val_acc: 0.3062\n",
      "Epoch 78/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3628 - acc: 0.3130 - val_loss: 1.3528 - val_acc: 0.2998\n",
      "Epoch 79/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3602 - acc: 0.3316 - val_loss: 1.3597 - val_acc: 0.3084\n",
      "Epoch 80/100\n",
      "2645/2645 [==============================] - 0s 78us/step - loss: 1.3582 - acc: 0.3289 - val_loss: 1.3543 - val_acc: 0.3062\n",
      "Epoch 81/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3676 - acc: 0.3221 - val_loss: 1.3552 - val_acc: 0.3084\n",
      "Epoch 82/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3605 - acc: 0.3327 - val_loss: 1.3523 - val_acc: 0.3126\n",
      "Epoch 83/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3646 - acc: 0.3168 - val_loss: 1.3546 - val_acc: 0.2912\n",
      "Epoch 84/100\n",
      "2645/2645 [==============================] - 0s 106us/step - loss: 1.3585 - acc: 0.3323 - val_loss: 1.3528 - val_acc: 0.2998\n",
      "Epoch 85/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3584 - acc: 0.3282 - val_loss: 1.3604 - val_acc: 0.2976\n",
      "Epoch 86/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3627 - acc: 0.3191 - val_loss: 1.3492 - val_acc: 0.2955\n",
      "Epoch 87/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3624 - acc: 0.3251 - val_loss: 1.3520 - val_acc: 0.2805\n",
      "Epoch 88/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3653 - acc: 0.3255 - val_loss: 1.3507 - val_acc: 0.3041\n",
      "Epoch 89/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3641 - acc: 0.3259 - val_loss: 1.3557 - val_acc: 0.2976\n",
      "Epoch 90/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3627 - acc: 0.3274 - val_loss: 1.3607 - val_acc: 0.2762\n",
      "Epoch 91/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3606 - acc: 0.3214 - val_loss: 1.3635 - val_acc: 0.2762\n",
      "Epoch 92/100\n",
      "2645/2645 [==============================] - 0s 106us/step - loss: 1.3637 - acc: 0.3236 - val_loss: 1.3554 - val_acc: 0.2805\n",
      "Epoch 93/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3621 - acc: 0.3331 - val_loss: 1.3588 - val_acc: 0.2869\n",
      "Epoch 94/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3603 - acc: 0.3130 - val_loss: 1.3512 - val_acc: 0.2976\n",
      "Epoch 95/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3663 - acc: 0.3176 - val_loss: 1.3527 - val_acc: 0.3019\n",
      "Epoch 96/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3601 - acc: 0.3221 - val_loss: 1.3615 - val_acc: 0.3019\n",
      "Epoch 97/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3601 - acc: 0.3248 - val_loss: 1.3641 - val_acc: 0.3084\n",
      "Epoch 98/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3576 - acc: 0.3342 - val_loss: 1.3642 - val_acc: 0.3105\n",
      "Epoch 99/100\n",
      "2645/2645 [==============================] - 0s 107us/step - loss: 1.3623 - acc: 0.3327 - val_loss: 1.3669 - val_acc: 0.2976\n",
      "Epoch 100/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3609 - acc: 0.3331 - val_loss: 1.3766 - val_acc: 0.3041\n"
     ]
    }
   ],
   "source": [
    "#target_test = categorical_labels[stop:]\n",
    "#target_train = categorical_labels[:stop]\n",
    "# we actually fit the model here\n",
    "history = model.fit(top_X_train,\n",
    "                    target_train,\n",
    "                    epochs=100,\n",
    "                    validation_split=0.15,\n",
    "                    callbacks=cb,\n",
    "                    batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label = 'training loss')\n",
    "plt.plot(history.history['val_loss'], label = 'validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2645 samples, validate on 467 samples\n",
      "Epoch 1/100\n",
      "2645/2645 [==============================] - 0s 100us/step - loss: 1.3597 - acc: 0.3267 - val_loss: 1.3933 - val_acc: 0.2848\n",
      "Epoch 2/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3601 - acc: 0.3301 - val_loss: 1.4073 - val_acc: 0.2698\n",
      "Epoch 3/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3571 - acc: 0.3391 - val_loss: 1.4067 - val_acc: 0.2441\n",
      "Epoch 4/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3622 - acc: 0.3353 - val_loss: 1.4056 - val_acc: 0.2698\n",
      "Epoch 5/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3548 - acc: 0.3323 - val_loss: 1.4057 - val_acc: 0.2612\n",
      "Epoch 6/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3600 - acc: 0.3327 - val_loss: 1.4061 - val_acc: 0.2741\n",
      "Epoch 7/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3611 - acc: 0.3263 - val_loss: 1.3981 - val_acc: 0.2762\n",
      "Epoch 8/100\n",
      "2645/2645 [==============================] - 0s 107us/step - loss: 1.3611 - acc: 0.3251 - val_loss: 1.3883 - val_acc: 0.2827\n",
      "Epoch 9/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3585 - acc: 0.3301 - val_loss: 1.3903 - val_acc: 0.2655\n",
      "Epoch 10/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3602 - acc: 0.3214 - val_loss: 1.3872 - val_acc: 0.2591\n",
      "Epoch 11/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3580 - acc: 0.3384 - val_loss: 1.3947 - val_acc: 0.2612\n",
      "Epoch 12/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3643 - acc: 0.3270 - val_loss: 1.4091 - val_acc: 0.2698\n",
      "Epoch 13/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3583 - acc: 0.3338 - val_loss: 1.4052 - val_acc: 0.2805\n",
      "Epoch 14/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3616 - acc: 0.3301 - val_loss: 1.3986 - val_acc: 0.2848\n",
      "Epoch 15/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3623 - acc: 0.3259 - val_loss: 1.3918 - val_acc: 0.2827\n",
      "Epoch 16/100\n",
      "2645/2645 [==============================] - 0s 106us/step - loss: 1.3591 - acc: 0.3316 - val_loss: 1.4034 - val_acc: 0.2677\n",
      "Epoch 17/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3563 - acc: 0.3342 - val_loss: 1.4009 - val_acc: 0.2634\n",
      "Epoch 18/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3562 - acc: 0.3350 - val_loss: 1.3889 - val_acc: 0.2848\n",
      "Epoch 19/100\n",
      "2645/2645 [==============================] - 0s 78us/step - loss: 1.3587 - acc: 0.3316 - val_loss: 1.3983 - val_acc: 0.2827\n",
      "Epoch 20/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3610 - acc: 0.3308 - val_loss: 1.3903 - val_acc: 0.2805\n",
      "Epoch 21/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3569 - acc: 0.3274 - val_loss: 1.3859 - val_acc: 0.2677\n",
      "Epoch 22/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3580 - acc: 0.3240 - val_loss: 1.3875 - val_acc: 0.2719\n",
      "Epoch 23/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3568 - acc: 0.3350 - val_loss: 1.3954 - val_acc: 0.2612\n",
      "Epoch 24/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3559 - acc: 0.3285 - val_loss: 1.3967 - val_acc: 0.2698\n",
      "Epoch 25/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3542 - acc: 0.3357 - val_loss: 1.4086 - val_acc: 0.2463\n",
      "Epoch 26/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3578 - acc: 0.3388 - val_loss: 1.4270 - val_acc: 0.2570\n",
      "Epoch 27/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3596 - acc: 0.3376 - val_loss: 1.4184 - val_acc: 0.2505\n",
      "Epoch 28/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3568 - acc: 0.3293 - val_loss: 1.4012 - val_acc: 0.2612\n",
      "Epoch 29/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3591 - acc: 0.3240 - val_loss: 1.3995 - val_acc: 0.2677\n",
      "Epoch 30/100\n",
      "2645/2645 [==============================] - 0s 106us/step - loss: 1.3579 - acc: 0.3357 - val_loss: 1.4214 - val_acc: 0.2570\n",
      "Epoch 31/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3589 - acc: 0.3323 - val_loss: 1.4125 - val_acc: 0.2698\n",
      "Epoch 32/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3572 - acc: 0.3229 - val_loss: 1.3994 - val_acc: 0.2719\n",
      "Epoch 33/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3573 - acc: 0.3346 - val_loss: 1.4010 - val_acc: 0.2634\n",
      "Epoch 34/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3603 - acc: 0.3353 - val_loss: 1.3901 - val_acc: 0.2548\n",
      "Epoch 35/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3614 - acc: 0.3369 - val_loss: 1.4003 - val_acc: 0.2570\n",
      "Epoch 36/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3602 - acc: 0.3225 - val_loss: 1.4003 - val_acc: 0.2591\n",
      "Epoch 37/100\n",
      "2645/2645 [==============================] - 0s 107us/step - loss: 1.3587 - acc: 0.3233 - val_loss: 1.3895 - val_acc: 0.2934\n",
      "Epoch 38/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3537 - acc: 0.3251 - val_loss: 1.3891 - val_acc: 0.2634\n",
      "Epoch 39/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3594 - acc: 0.3331 - val_loss: 1.3966 - val_acc: 0.2698\n",
      "Epoch 40/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3583 - acc: 0.3172 - val_loss: 1.3940 - val_acc: 0.2612\n",
      "Epoch 41/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3554 - acc: 0.3353 - val_loss: 1.3933 - val_acc: 0.2591\n",
      "Epoch 42/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3561 - acc: 0.3372 - val_loss: 1.4042 - val_acc: 0.2591\n",
      "Epoch 43/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3579 - acc: 0.3395 - val_loss: 1.4010 - val_acc: 0.2719\n",
      "Epoch 44/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3562 - acc: 0.3278 - val_loss: 1.4014 - val_acc: 0.2762\n",
      "Epoch 45/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3563 - acc: 0.3384 - val_loss: 1.4020 - val_acc: 0.2655\n",
      "Epoch 46/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3589 - acc: 0.3263 - val_loss: 1.3945 - val_acc: 0.2677\n",
      "Epoch 47/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3535 - acc: 0.3456 - val_loss: 1.3996 - val_acc: 0.2484\n",
      "Epoch 48/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3609 - acc: 0.3304 - val_loss: 1.3955 - val_acc: 0.2591\n",
      "Epoch 49/100\n",
      "2645/2645 [==============================] - 0s 78us/step - loss: 1.3570 - acc: 0.3448 - val_loss: 1.3978 - val_acc: 0.2719\n",
      "Epoch 50/100\n",
      "2645/2645 [==============================] - 0s 73us/step - loss: 1.3607 - acc: 0.3357 - val_loss: 1.3854 - val_acc: 0.2741\n",
      "Epoch 51/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3547 - acc: 0.3452 - val_loss: 1.3909 - val_acc: 0.2570\n",
      "Epoch 52/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3575 - acc: 0.3270 - val_loss: 1.3896 - val_acc: 0.2484\n",
      "Epoch 53/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3564 - acc: 0.3388 - val_loss: 1.3917 - val_acc: 0.2505\n",
      "Epoch 54/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3552 - acc: 0.3316 - val_loss: 1.4060 - val_acc: 0.2527\n",
      "Epoch 55/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3552 - acc: 0.3263 - val_loss: 1.4014 - val_acc: 0.2484\n",
      "Epoch 56/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3584 - acc: 0.3293 - val_loss: 1.3891 - val_acc: 0.2527\n",
      "Epoch 57/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3552 - acc: 0.3425 - val_loss: 1.3983 - val_acc: 0.2655\n",
      "Epoch 58/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3585 - acc: 0.3353 - val_loss: 1.3914 - val_acc: 0.2612\n",
      "Epoch 59/100\n",
      "2645/2645 [==============================] - 0s 106us/step - loss: 1.3513 - acc: 0.3388 - val_loss: 1.3918 - val_acc: 0.2634\n",
      "Epoch 60/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3591 - acc: 0.3312 - val_loss: 1.3879 - val_acc: 0.2762\n",
      "Epoch 61/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3569 - acc: 0.3395 - val_loss: 1.3899 - val_acc: 0.2784\n",
      "Epoch 62/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3520 - acc: 0.3357 - val_loss: 1.3943 - val_acc: 0.2612\n",
      "Epoch 63/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3594 - acc: 0.3406 - val_loss: 1.4027 - val_acc: 0.2612\n",
      "Epoch 64/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3561 - acc: 0.3289 - val_loss: 1.4108 - val_acc: 0.2548\n",
      "Epoch 65/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3555 - acc: 0.3353 - val_loss: 1.4093 - val_acc: 0.2677\n",
      "Epoch 66/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3534 - acc: 0.3403 - val_loss: 1.4106 - val_acc: 0.2655\n",
      "Epoch 67/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3556 - acc: 0.3312 - val_loss: 1.4006 - val_acc: 0.2698\n",
      "Epoch 68/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3568 - acc: 0.3293 - val_loss: 1.3983 - val_acc: 0.2741\n",
      "Epoch 69/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3591 - acc: 0.3384 - val_loss: 1.4016 - val_acc: 0.2784\n",
      "Epoch 70/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3575 - acc: 0.3293 - val_loss: 1.4085 - val_acc: 0.2698\n",
      "Epoch 71/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3569 - acc: 0.3285 - val_loss: 1.4060 - val_acc: 0.2570\n",
      "Epoch 72/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3551 - acc: 0.3342 - val_loss: 1.4110 - val_acc: 0.2570\n",
      "Epoch 73/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3548 - acc: 0.3418 - val_loss: 1.4127 - val_acc: 0.2655\n",
      "Epoch 74/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3584 - acc: 0.3395 - val_loss: 1.4079 - val_acc: 0.2612\n",
      "Epoch 75/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3543 - acc: 0.3319 - val_loss: 1.4081 - val_acc: 0.2655\n",
      "Epoch 76/100\n",
      "2645/2645 [==============================] - 0s 106us/step - loss: 1.3537 - acc: 0.3316 - val_loss: 1.4114 - val_acc: 0.2655\n",
      "Epoch 77/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3551 - acc: 0.3437 - val_loss: 1.4123 - val_acc: 0.2591\n",
      "Epoch 78/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3559 - acc: 0.3316 - val_loss: 1.4159 - val_acc: 0.2655\n",
      "Epoch 79/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3555 - acc: 0.3312 - val_loss: 1.4192 - val_acc: 0.2698\n",
      "Epoch 80/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3584 - acc: 0.3372 - val_loss: 1.4174 - val_acc: 0.2741\n",
      "Epoch 81/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3522 - acc: 0.3410 - val_loss: 1.4152 - val_acc: 0.2719\n",
      "Epoch 82/100\n",
      "2645/2645 [==============================] - 0s 107us/step - loss: 1.3578 - acc: 0.3342 - val_loss: 1.4170 - val_acc: 0.2634\n",
      "Epoch 83/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3538 - acc: 0.3440 - val_loss: 1.4213 - val_acc: 0.2719\n",
      "Epoch 84/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3573 - acc: 0.3406 - val_loss: 1.4144 - val_acc: 0.2634\n",
      "Epoch 85/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3529 - acc: 0.3365 - val_loss: 1.4225 - val_acc: 0.2505\n",
      "Epoch 86/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3510 - acc: 0.3474 - val_loss: 1.4211 - val_acc: 0.2612\n",
      "Epoch 87/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3571 - acc: 0.3384 - val_loss: 1.4266 - val_acc: 0.2677\n",
      "Epoch 88/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3543 - acc: 0.3388 - val_loss: 1.4323 - val_acc: 0.2634\n",
      "Epoch 89/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3555 - acc: 0.3380 - val_loss: 1.4110 - val_acc: 0.2762\n",
      "Epoch 90/100\n",
      "2645/2645 [==============================] - 0s 106us/step - loss: 1.3555 - acc: 0.3384 - val_loss: 1.4057 - val_acc: 0.2527\n",
      "Epoch 91/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3595 - acc: 0.3372 - val_loss: 1.4058 - val_acc: 0.2591\n",
      "Epoch 92/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3580 - acc: 0.3338 - val_loss: 1.4146 - val_acc: 0.2677\n",
      "Epoch 93/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3544 - acc: 0.3440 - val_loss: 1.4292 - val_acc: 0.2484\n",
      "Epoch 94/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3603 - acc: 0.3293 - val_loss: 1.4212 - val_acc: 0.2698\n",
      "Epoch 95/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3533 - acc: 0.3384 - val_loss: 1.4079 - val_acc: 0.2719\n",
      "Epoch 96/100\n",
      "2645/2645 [==============================] - 0s 77us/step - loss: 1.3551 - acc: 0.3365 - val_loss: 1.4101 - val_acc: 0.2677\n",
      "Epoch 97/100\n",
      "2645/2645 [==============================] - 0s 105us/step - loss: 1.3537 - acc: 0.3463 - val_loss: 1.4100 - val_acc: 0.2698\n",
      "Epoch 98/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3580 - acc: 0.3418 - val_loss: 1.4063 - val_acc: 0.2698\n",
      "Epoch 99/100\n",
      "2645/2645 [==============================] - 0s 75us/step - loss: 1.3568 - acc: 0.3406 - val_loss: 1.4128 - val_acc: 0.2591\n",
      "Epoch 100/100\n",
      "2645/2645 [==============================] - 0s 76us/step - loss: 1.3583 - acc: 0.3376 - val_loss: 1.4072 - val_acc: 0.2762\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass-multioutput and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-05acc4e77d9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtarget_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_X_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m print(\"Accuracy:\",metrics.accuracy_score(target_test_int, target_pred),'\\n'\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0;34m'Cohans Kappa:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcohen_kappa_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_test_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0;34m'Train ACC:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_train_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mdiffering_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 81\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass-multioutput and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "history = model.fit(top_X_train,\n",
    "                    target_train,\n",
    "                    epochs=100,\n",
    "                    validation_split=0.15,\n",
    "                    callbacks=cb,\n",
    "                    batch_size=200)\n",
    "target_pred = model.predict(top_X_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(target_test_int, target_pred),'\\n'\n",
    "      'Cohans Kappa:', metrics.cohen_kappa_score(target_test_int, target_pred),'\\n'\n",
    "      'Train ACC:', metrics.accuracy_score(target_train_int, target_pred), '\\n'\n",
    "      \"Confusion Matrix:\",'\\n',\n",
    "      metrics.confusion_matrix(target_test, target_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18847424, 0.20657857, 0.26712638, 0.3378208 ],\n",
       "       [0.19909625, 0.21071649, 0.25854376, 0.3316435 ],\n",
       "       [0.20781128, 0.21539953, 0.2511966 , 0.32559255],\n",
       "       ...,\n",
       "       [0.19012035, 0.07394086, 0.22188143, 0.51405734],\n",
       "       [0.35369614, 0.09347692, 0.19233045, 0.3604965 ],\n",
       "       [0.52052873, 0.10334933, 0.14951088, 0.22661106]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 0, 1],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " [0, 0, 1, 0],\n",
       " [0, 1, 0, 0],\n",
       " [1, 0, 0, 0],\n",
       " ...]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_train_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['abv_1.07', 'abv_1.07', 'abv_1.07', 'abv_1.04', 'abv_1.02',\n",
       "       'bel_1.02', 'abv_1.07', 'abv_1.07', 'abv_1.07', 'abv_1.07'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = df_cln[target_name].values\n",
    "targ_test = targets[stop:]\n",
    "targ_train = targets[:stop]\n",
    "targ_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'History' object has no attribute 'predict_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-03cbf1c05088>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'History' object has no attribute 'predict_classes'"
     ]
    }
   ],
   "source": [
    "history.predict_classes(feature_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_X_train,\n",
    "                    target_train,\n",
    "                    epochs=100,\n",
    "                    validation_split=0.15,\n",
    "                    callbacks=cb,\n",
    "                    batch_size=200"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
